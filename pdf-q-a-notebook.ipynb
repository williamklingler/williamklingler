{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Chat with your PDF files using LlamaIndex, Astra DB (Apache Cassandra), and Gradient's open-source models, including LLama2 and Streamlit, all designed for seamless interaction with PDF files.\n",
        "\n",
        "[**Link to my YouTube Channel**](https://www.youtube.com/BhaveshBhatt8791?sub_confirmation=1)\n",
        "\n",
        "Click on the link below to open a Colab version of the notebook. You will be able to create your own version."
      ],
      "metadata": {
        "id": "sbzI6KKiDv78"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhattbhavesh91//pdf-q-a-llamaindex-llama2/blob/main/pdf-q-a-notebook.ipynb\" target=\"_blank\"><img height=\"40\" alt=\"Run your own notebook in Colab\" src = \"https://colab.research.google.com/assets/colab-badge.svg\"></a>"
      ],
      "metadata": {
        "id": "iW1MO9bXD964"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "XaiKzsSw4FBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q cassandra-driver\n",
        "!pip install -q cassio>=0.1.1\n",
        "!pip install -q gradientai --upgrade\n",
        "!pip install -q llama-index\n",
        "!pip install -q pypdf\n",
        "!pip install -q tiktoken==0.4.0"
      ],
      "metadata": {
        "id": "wak76xYYUdXE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b182a363-918f-4471-db1f-9bf8ca0214be"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.8/18.8 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.2/270.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.1/225.1 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.9/283.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import OS & JSON Modules"
      ],
      "metadata": {
        "id": "2iILdU7U4Hya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['GRADIENT_ACCESS_TOKEN'] = 'Enter your GRADIENT ACCESS TOKEN'\n",
        "os.environ['GRADIENT_WORKSPACE_ID'] =  'Enter your GRADIENT WORKSPACE ID'"
      ],
      "metadata": {
        "id": "U02ytLrPA2rG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Cassandra & llama Index"
      ],
      "metadata": {
        "id": "u3IS4xyg4N63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from cassandra.auth import PlainTextAuthProvider\n",
        "from cassandra.cluster import Cluster\n",
        "from llama_index import ServiceContext\n",
        "from llama_index import set_global_service_context\n",
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader, StorageContext\n",
        "from llama_index.embeddings import GradientEmbedding\n",
        "from llama_index.llms import GradientBaseModelLLM\n",
        "from llama_index.vector_stores import CassandraVectorStore"
      ],
      "metadata": {
        "id": "GhZ1NMr2z3vF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cassandra\n",
        "print (cassandra.__version__)"
      ],
      "metadata": {
        "id": "Q28fIUq91zED",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "738f5af6-b441-43b7-c450-72a597c8fb4d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.29.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connect to the VectorDB"
      ],
      "metadata": {
        "id": "cuio1UWlEMkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "secrets = {\n",
        "  \"clientId\": userdata.get('astraclientId'),\n",
        "  \"secret\": userdata.get('astratoken'),\n",
        "  \"token\": userdata.get('secret')\n",
        "}\n",
        "\n",
        "CLIENT_ID = secrets[\"clientId\"]\n",
        "CLIENT_SECRET = secrets[\"secret\"]\n",
        "\n",
        "auth_provider = PlainTextAuthProvider(CLIENT_ID, CLIENT_SECRET)\n",
        "cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)\n",
        "session = cluster.connect()\n",
        "\n",
        "row = session.execute(\"select release_version from system.local\").one()\n",
        "if row:\n",
        "  print(row[0])\n",
        "else:\n",
        "  print(\"An error occurred.\")"
      ],
      "metadata": {
        "id": "jYn5am9c1zGS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "8205228d-a62f-4bcd-d703-8eedeac1f015"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'bhavesh_astra_test-token.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-42c51b74e2dd>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# This token json file is autogenerated when you donwload your token,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# if yours is different update the file name below\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bhavesh_astra_test-token.json\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0msecrets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'bhavesh_astra_test-token.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the Gradient's Model Adapter for LLAMA-2"
      ],
      "metadata": {
        "id": "1I8T-kjh4VoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = GradientBaseModelLLM(\n",
        "    base_model_slug=\"llama2-7b-chat\",\n",
        "    max_tokens=400,\n",
        ")"
      ],
      "metadata": {
        "id": "EksBmgQt36_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configure Gradient embeddings"
      ],
      "metadata": {
        "id": "CPTqOwXZ4aLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_model = GradientEmbedding(\n",
        "    gradient_access_token = os.environ[\"GRADIENT_ACCESS_TOKEN\"],\n",
        "    gradient_workspace_id = os.environ[\"GRADIENT_WORKSPACE_ID\"],\n",
        "    gradient_model_slug=\"bge-large\",\n",
        ")"
      ],
      "metadata": {
        "id": "38_uBhIy2XtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "service_context = ServiceContext.from_defaults(\n",
        "    llm = llm,\n",
        "    embed_model = embed_model,\n",
        "    chunk_size=256,\n",
        ")\n",
        "\n",
        "set_global_service_context(service_context)"
      ],
      "metadata": {
        "id": "mf7hu0cE2VKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the PDFs"
      ],
      "metadata": {
        "id": "T3AEoS-t4t6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader(\"/content/Documents\").load_data()\n",
        "print(f\"Loaded {len(documents)} document(s).\")"
      ],
      "metadata": {
        "id": "KFXZvFaQ25kE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and Query Index"
      ],
      "metadata": {
        "id": "Ns8r13uw4vko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(documents,\n",
        "                                        service_context=service_context)\n",
        "query_engine = index.as_query_engine()"
      ],
      "metadata": {
        "id": "YT9wD9sQ25nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"What is Positional Encoding?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "ZVYy24TR2VSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nRyW0E5N4DiX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}